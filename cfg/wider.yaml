input_shape: [176, 208, 176]
latent_dim: 354
layers:
 conv1:
  channels: 32
  kernel_size: 3
  stride: 2
  padding: 1
  pool_size: 0
  pool_stride: 0
  bias: yes
  batch_norm: yes
  activation: 'relu'
 conv2:
  channels: 64
  kernel_size: 3
  stride: 2
  padding: 1
  pool_size: 0
  pool_stride: 0
  bias: yes
  batch_norm: yes
  activation: 'relu'
 conv3:
  channels: 128
  kernel_size: 3
  stride: 2
  padding: 1
  pool_size: 0
  pool_stride: 0
  bias: yes
  batch_norm: yes
  activation: 'relu'
 conv4:
  channels: 128
  kernel_size: 3
  stride: 1
  padding: 0
  pool_size: 0
  pool_stride: 0
  bias: yes
  batch_norm: yes
  activation: 'relu'
 conv5:
  channels: 256
  kernel_size: 3
  stride: 2
  padding: 1
  pool_size: 0
  pool_stride: 0
  bias: yes
  batch_norm: yes
  activation: 'relu'
 conv6:
  channels: 256
  kernel_size: 3
  stride: 1
  padding: 0
  pool_size: 0
  pool_stride: 0
  bias: yes
  batch_norm: yes
  activation: 'relu'
 conv7:
  channels: 512
  kernel_size: 3
  stride: 2
  padding: 1
  pool_size: 0
  pool_stride: 0
  bias: yes
  batch_norm: yes
  activation: 'relu'
 conv8:
  channels: 512
  kernel_size: 3
  stride: 1
  padding: 0
  pool_size: 0
  pool_stride: 0
  bias: yes
  batch_norm: yes
  activation: 'relu'

conditional_dim: 0
invariant: no
lr: 0.0004
min_lr: 0.0004
optimizer: 'AdamW'
momentum: 0.9
weight_decay: 0.01
losses_weights:
  reconstruction: 1.0
  prior: 0.00001
  marginal: 0.0
